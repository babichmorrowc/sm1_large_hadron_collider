---
title: "Maximising AMS by classifying Higgs Boson Data - a love story"
author: "Kieran Morris, Cecina Babich Morrow and Daniella Montgomery"
date: "2023-11-07"
output: html_document
---

```{r include = FALSE}
library(tidyverse)
library(here)
library(skimr)
library(ggplot2)
library(purrr)
library(ggpubr) #for ggarrange
library(DT) #datatables
library(tfprobability) #kldivergence package
```

```{r, include = FALSE}
higgs_data_orig <- read_csv("../data/atlas-higgs-challenge-2014-v2.csv",show_col_types = F)
higgs_data <- higgs_data_orig%>%mutate(across(where(is.numeric), ~na_if(.x, -999)))
summary_OfHiggs <- skim(higgs_data)
```

# Introduction

## The Objective

Our data set consists of observations from the Large Hadron Collider at CERN over the year of 2012. It is ordered data of the form $\{(\mathbf{x}_i,y_i,w_i)\}_{i \in D}$ where $\mathbf{x} \in \mathbb{R}^{30}$; $y \in \{\mathbf{b},\mathbf{s}\}$ and $w \in [0,1]$ is a weight which measures the intensity of each data point.

Each $\mathbf{x}_i$ is the collection of observables about each event and each $y_i$ is the categorization of the event as either 'Background' $(= \mathbf{b})$ and 'Signal' $=(\mathbf{s})$. We define $$
\mathcal{S} = \{i: y_i = \mathbf{s}\} \text{ and } \mathcal{B} = \{i: y_i  =\mathbf{b}\}
$$ with $n_s = \mid \mathcal{S} \mid$ and $n_b = \mid \mathcal{B} \mid$ respectively. The weight variable is not to be used to train the classifier $f$ in any way, it is only to compute the AMS, which is our measure of the accuracy of our classifier $f$. Speaking of $f$, we define $\hat{f} = \{i: f(\mathbf{x}_i) = \mathbf{s}\}$, i.e the points labelled as signals by $f$ and using this we define

$$
s = \sum_{i \in \mathcal{S}\cap \hat{f}}w_i \text{ and }  b = \sum_{i \in \mathcal{B}\cap \hat{f}}w_i
$$

i.e $s$ and $b$ are the weighted $\textit{true positives}$ and $\textit{false positives}$ respectively. Finally we define the AMS function is defined as follows:

$$
\text{AMS}(f) = \sqrt{2\left(s+b+b_{reg}\ln \left (1+ \frac{s}{b+b_{reg}} \right) -s  \right)}
$$

Where $b$ and $s$ are as above and $b_{reg}$ is a normalizing constant. Notice that this is computed over a data set $D$, so if we wish to compute this over some training or test set we must re-normalize the weights.

Our task is to create some classifier $f$ which maximizes the AMS.

## The Method

As we have high dimensional data with varied distributions we will perform a lot of preliminary computation before beginning classification. The following is a loose structure of our method for tackling this problem:

1.  Eliminate any obviously redundant variables such as meta-data.
2.  Analyse and deal with any missing data.
3.  Study the variables to find which have minimal impact on classification.
4.  Perform logistic regression under different assumptions, i.e different variables or feature transforms.
5.  Study the impact on the AMS.

# Preliminary Statistics

## Types of Data

We have a few unique types of data to consider:

-   Variables `KaggleSet` and `KaggleWeight` can be ignored as they denote which data points were in the provided Kaggle challenge and their relative weights

-   The discrete classification variable `Label` which takes values in $\{b,s\}$

-   The continuous variable `Weight`, which we only use to compute the AMS

-   The discrete variable `PRI_jet_num` which denotes the amount of jets from each event and takes values in $\{0,1,2,3\}$

-   All other variables are either direct measurements - or computed from direct measurements from the LHC and take continuous values.

## Co-Dependency of Data

Multiple of our variables are dependent on one or another. Since we are classifying, we want to remove as many dimensions as possible. We have the following co-dependencies:

-   `PRI_jet_all_pt = PRI_jet_leading_pt + PRI_jet_subleading_pt`
-   `DER_pt_ratio_lep_tau = PRI_lep_pt/PRI_tau_pt`
-   Jet-related variables depend on `PRI_jet_num` (see next section for more)

This allows us to reduce the dimension of our data by removing some redundant subset of our variables. However there will be a trade off between dimension reduction and detail. If we are liberal we can remove 2+2+10 = 14 dimensions, however this would probably remove too much information.

## Missing Data

### What is missing and why?

Below are the variables which contain undefined values. By convention it was provided to use with values `-999` which were all out of range for the observations, however we have converted such values to `NA` to prevent `-999` values from skewing preliminary statistics.

```{r,echo = FALSE}
variables_WithError <- filter(yank(summary_OfHiggs,"numeric"), complete_rate != 1)
ve <- datatable(variables_WithError[1:3],options = list(pageLength = 11))
ve
```

Notice that every column besides `DER_mass_MMC` is associated to jets, and in those, we only have two values for `completion_rate`. In fact these are a result of different values of `PRI_jet_num`:

-   0 jets: All jet variables were missing data.

-   1 jet: only `PRI_leading_pt`,`PRI_leading_eta` and `PRI_leading_phi` have data.

-   2 or more jets: All jet variables have data.

The above result can be found statistically or in the handbook for the variables provided with the Kaggle challenge. Unfortunately `DER_mass_MMC` does not have such an explanation and may be a result of some event during measurement. However it is still assigned the same `-999` value as the other missing data.

### Impact of the Missing Data

Despite understanding the cause of (most of) our missing data, we still don't know the impact, the following section is dedicated to understanding the correlation between the missing data from each variable and its classification. Take `DER_mass_MMC` for example, if we find that the missing data is distributed uniformly across $y_i \in \{b,s\}$ then the appearance of `NA` has no impact on classification. This would allow us to run a regression algorithm to let us interpolate missing values and possibly improve classification. However if it is disproportionately skewed towards $b$ signals then the missing data will assist us in classification.

Below we compute the percentage of `NA` data observed in Background and Signal data (relative to the amount of Background/Signal respectively) for our 11 incomplete columns. If this missing data is distributed uniformly across Background and Signal then the percentages should be very close.

```{r, echo = FALSE}
num_DataPoints <- length(higgs_data$Label) #compute number of data points
num_Background <- length(Filter(function(x){if (x =="b"){return(T)}else{return(F)}},higgs_data$Label))#compute number of background
num_Signal <- num_DataPoints - num_Background#number of signals

variables_WithError <- filter(summary_OfHiggs, complete_rate != 1)#Finding the columns which contain undefined elements

Background <- filter(higgs_data, Label == "b")[c(variables_WithError$skim_variable)]#filtering rows by background
Signal <- filter(higgs_data, Label =="s")[c(variables_WithError$skim_variable)]#filter rows by signal

missing_GivenB <- apply(Background,2,function(x){return(round(100*sum(is.na(x))/length(x),digits = 4))})#calculate percentage missing data for background and signal
missing_GivenS <- apply(Signal,2,function(x){return(round(100*sum(is.na(x))/length(x),digits = 4))})

df <- datatable(data.frame(missing_GivenS,missing_GivenB,missing_GivenB-missing_GivenS),col = c("Signal","Background","Difference"),caption = "% of Missing Background and Signal Data",options = list(pageLength = 11))

df
```

Notice that `Signal` consistently has $13-18 \%$ less missing data than `Background`. This means that in both the `DER_mass_MMC` and jet-variables cases, the amount of missing data is indicative of a classification. In particular we cannot use regression to simulate missing data as we would lose valuable classifying information. Because of this, we decide to keep the `-999` values when constructing our classifier.

## Variable Analysis

### The Distributions

To get an understanding of the behavior of variable given their classification, we plotted their densities, along with the Mutual Information $I$, which is a positive definite measure of difference between two distributions. The entire list can be found in the appendix but here are a few of note:

```{r, warning=FALSE,message = FALSE, echo = FALSE,fig.align="center",fig.width = 14,fig.height= 10}
plots <- function(Parameter){
  pl <- ggplot(higgs_data, aes(x= get(Parameter),y = ..scaled.., fill = Label)) +
    geom_density(alpha = 0.5) +
    labs(x = Parameter)
  return(pl)
}

cols_ToPlot <- c(colnames(higgs_data)[2:3],colnames(higgs_data)[29:30]) #remove jet_num
plot_list <- lapply(cols_ToPlot,plots)
ggarrange(plotlist = plot_list,ncol = 2, nrow = 2, align = "v", common.legend = TRUE)
```

Note that the data for these plots have `NA` instead of `-999` for missing data, again see appendix for plots with `-999` included. Some key takeaways from these plots are:

-   Many of our parameters are over the range $[-\pi,\pi]$ as they correspond to the angles observed in interactions.

-   We see that `PRI_tau_phi`,`PRI_lep_phi`, `PRI_met_phi`,`PRI_jet_leading_phi`and `PRI_jet_subleading_phi` are uniformly distributed and that there is very little variation between the classifications. Additionally all except `PRI_jet_subleading_phi` are unaffected when we include the `-999` values. This leads us to consider rejecting them from our classification, reducing the dimension by up to 5.

-   The parameters `PRI_tau_eta` and `PRI_lep_eta` have exotic distributions which vary with classification.

-   We have many `single peak` distributions such as `DER_mass_MMC`,`DER_mass_transverse_met_lep` and `DER_pt_ratio_lep_tau` which have clear variation between the classes.

-   We can see that the Jet-related variables have clear variation between the classes, reinforcing our need to use them and their corresponding `-999` values.

### K-L Divergence between b and s

To formalize our notion of the distributions changing between different classes we can compute the K-L divergence between the two distributions for each variable. Recall that the K-L divergence gives a positive definite measure of distance between two distributions. Like with the pdf plots, a full table of K-L divergence can be found in the appendix, but here are a few of note:

# Fisher Discriminant Analysis

In order to get an idea how separated our data is we perform FDA, in particular we want to find out how rejecting certain families of variables affects the separation of our classes. In turn telling us what we can reject. As we discussed in the previous section, multiple variables related to measured angles were uniformly distributed and did vary between classes either, therefore we will split our FDA into two cases to see the difference. Additionally we will remove multiple codependent variables and compare how this affects the result.

```{r}
#source("clean_data.R")
#source("fda.R")
```

## Keeping Angle Variables

### Including missing data

```{r}
ldahist(data = fda_pred$x[,1], g = fda_training$Label) #FDA when we include the the columns with #missing data
```

## Excluding missing data

```{r}
fda_nomissing <- lda(Label ~ ., fda_training_nomissing) #FDA when we exclude the columns with missing data
```

# Logistic Regression

## Implementation over two variables

# Appendix

## Density of each parameter without `-999` values

```{r setup, warning=FALSE,message = FALSE, echo = FALSE,fig.align="center",fig.width = 14,fig.height = 35}
plots <- function(Parameter){
  pl <- ggplot(higgs_data, aes(x= get(Parameter),y= after_stat(scaled), fill = Label)) +
    geom_density(alpha = 0.5) +
    labs(x = Parameter)
  return(pl)
}

plots2 <- function(Parameter){
  pl <- ggplot(higgs_data_orig, aes(x= get(Parameter),y=..scaled.., fill = Label)) +
    geom_density(alpha = 0.5) +
    labs(x = Parameter)
  return(pl)
}


cols_ToPlot <- c(colnames(higgs_data)[2:23],colnames(higgs_data)[25:31]) #remove jet_num
plot_list <- lapply(cols_ToPlot,plots)
ggarrange(plotlist = plot_list,ncol = 2, nrow = 15, align = "v", common.legend = TRUE)
```

## Density of each parameter with `-999` values

```{r,warning=FALSE,message = FALSE, echo = FALSE,fig.align="center",fig.width = 14,fig.height = 35}
cols_ToPlot2 <- c(colnames(higgs_data)[2:24],colnames(higgs_data)[25:31]) #remove jet_num
plot_list2 <- lapply(cols_ToPlot2,plots2)
ggarrange(plotlist = plot_list2,ncol = 2, nrow = 15, align = "v", common.legend = TRUE)
```
